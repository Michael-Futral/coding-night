{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Challenges\n",
    "\n",
    "For these exercises, we will be parsing information from [Quotes to Scrape](http://quotes.toscrape.com/) and [Books to Scrape](http://books.toscrape.com/). These sites are built and offered as web scraping testing grounds to learn different web scraping techniques. For this, we will focus on using [Requests](https://requests.readthedocs.io/en/master/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/).\n",
    "\n",
    "If you are looking for an extra challenge, here are a few things you can try:\n",
    "* Only parse content with:\n",
    "    * BeautifulSoup [`find()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find) and [`find_all()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all) methods\n",
    "    * [css selectors](https://www.w3.org/TR/selectors-4/) (BeautifulSoup)\n",
    "    * [path selectors](https://www.w3.org/TR/2017/REC-xpath-31-20170321/) (lxml)\n",
    "* Use [Selenium](https://selenium.dev/selenium/docs/api/py/) to scrape a [javascript version of Quotes to Scrape](http://quotes.toscrape.com/js/)\n",
    "* Use [Urllib](https://docs.python.org/3/library/urllib.html) from the Python Standard Library instead of Requests\n",
    "\n",
    "## Getting started\n",
    "\n",
    "To get started, we need to import the necessary modules. Unless you're attempting one of the challenges, we'll give you this one for free. However, you might find it useful to import some other modules later to help answer some questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "books_url = 'http://books.toscrape.com'\n",
    "quotes_url = 'http://quotes.toscrape.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Imports\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from tabulate import tabulate\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Page Content\n",
    "\n",
    "The first challenge is to [get](https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request) the page content. For now, let's just look at the Quotes site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(quotes_url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "What kind of information was returned in the response header?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(response.headers.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of information did we send in the request header?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(response.request.headers.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Data\n",
    "\n",
    "How many quotes are on the first page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "items = soup.select('div.quote')\n",
    "print(f\"The first page has {len(items)} quotes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagination\n",
    "\n",
    "How many pages of quotes are there to scrape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = '/page/1/'\n",
    "while next_page:\n",
    "    response = requests.get(urljoin(quotes_url, next_page))\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    try:\n",
    "        next_page = soup.select_one('li.next a')['href']\n",
    "    except TypeError:\n",
    "        break\n",
    "\n",
    "print(f\"There are {next_page.split('/')[2]} pages to scrape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many quotes are there to scrape? Store them in a list for further processing (reduces number of slow http requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = []\n",
    "\n",
    "next_page = '/page/1/'\n",
    "while next_page:\n",
    "    response = requests.get(urljoin(quotes_url, next_page))\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    quotes.extend(soup.select('div.quote'))\n",
    "    try:\n",
    "        next_page = soup.select_one('li.next a')['href']\n",
    "    except TypeError:\n",
    "        break\n",
    "        \n",
    "print(f'There are {len(quotes)} quotes to process.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags\n",
    "How many tags are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = Counter(\n",
    "    chain.from_iterable([\n",
    "        quote.select('a.tag') \n",
    "        for quote in quotes\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(f'There are {len(tags)} tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 20 tags? How many quotes does each tag have? What is it's url?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate([\n",
    "    {\n",
    "        'tag': tag.text,\n",
    "        'num': count,\n",
    "        'url': tag['href'],\n",
    "    }\n",
    "    for tag, count in tags.most_common(20)\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "How many authors are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = Counter([\n",
    "    quote.select_one('small.author').text\n",
    "    for quote in quotes\n",
    "])\n",
    "    \n",
    "print(f'There are {len(authors)} authors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 20 authors? How many quotes does each author have? What is their url?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_dict = {name: {'count': count} for name, count in authors.most_common()}\n",
    "for quote in quotes:\n",
    "    authors_dict[quote.select_one('small.author').text]['url'] = quote.select_one('span a')['href']\n",
    "\n",
    "for auth in authors_dict:\n",
    "    authors_dict[auth]['name'] = auth\n",
    "\n",
    "authors_list = list(authors_dict.values())\n",
    "\n",
    "print(tabulate(authors_list[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who is the oldest author? Who is the youngest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in authors_list:\n",
    "    response = requests.get(urljoin(quotes_url, author['url']))\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    author['birthday'] = datetime.strptime(\n",
    "        soup.select_one('.author-born-date').text,\n",
    "        '%B %d, %Y'\n",
    "    )\n",
    "    author['home'] = soup.select_one('.author-born-location').text[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_birthday = sorted(authors_list, key=lambda i: i['birthday'])\n",
    "\n",
    "print(f\"The oldest author is {by_birthday[0]['name']} born on {by_birthday[0]['birthday'].strftime('%B %d, %Y')}\")\n",
    "print(f\"The youngest author is {by_birthday[-1]['name']} born on {by_birthday[-1]['birthday'].strftime('%B %d, %Y')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where were the most authors born (by country)? Which countries have the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(\n",
    "    Counter([\n",
    "        auth['home'].split(', ')[-1] \n",
    "        for auth in authors_list]\n",
    "    ).most_common()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing JSON\n",
    "\n",
    "How many pages does it take to scrape the whole api? Use [params](https://requests.readthedocs.io/en/master/user/quickstart/#passing-parameters-in-urls) to build the query string.\n",
    "\n",
    "The api can be found at http://quotes.toscrape.com/api/quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_url_json = 'http://quotes.toscrape.com/api/quotes'\n",
    "\n",
    "next_page = 1\n",
    "while next_page:\n",
    "    response = requests.get(quotes_url_json, params={'page': next_page})\n",
    "    data = response.json()\n",
    "    if data['has_next']:\n",
    "        next_page += 1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print(f\"There are {next_page} pages to scrape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many quotes are available on the api?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = []\n",
    "\n",
    "next_page = 1\n",
    "while next_page:\n",
    "    response = requests.get(quotes_url_json, params={'page': next_page})\n",
    "    data = response.json()\n",
    "    quotes.extend(data['quotes'])\n",
    "    if data['has_next']:\n",
    "        next_page += 1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print(f'There are {len(quotes)} quotes to process.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique tags are in the api?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = Counter(\n",
    "    chain.from_iterable([\n",
    "        quote['tags']\n",
    "        for quote in quotes\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(f'There are {len(tags)} tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 20 tags? Do they match the previous result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(\n",
    "    tags.most_common(20)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique authors are in the api?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = Counter([\n",
    "    quote['author']['name']\n",
    "    for quote in quotes\n",
    "])\n",
    "    \n",
    "print(f'There are {len(authors)} authors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 20 authors? Do they match the previous result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(\n",
    "    authors.most_common(20)\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
